{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "certain-chest",
   "metadata": {},
   "source": [
    "# Deploying pipeline with custom environment\n",
    "\n",
    "This notebook will walk you through an end-to-end example deploying a Tempo pipeline, running on its own Conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-absolute",
   "metadata": {},
   "source": [
    "## Defining pipeline\n",
    "\n",
    "The first step will be to define our custom pipeline.\n",
    "This pipeline will access 2 models, stored remotely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honest-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tempo.serve.metadata import ModelFramework, KubernetesOptions\n",
    "from tempo.serve.model import Model\n",
    "from tempo.seldon.docker import SeldonDockerRuntime\n",
    "from tempo.kfserving.protocol import KFServingV2Protocol\n",
    "from tempo.serve.utils import pipeline, predictmethod\n",
    "from tempo.seldon.k8s import SeldonKubernetesRuntime\n",
    "from tempo.serve.utils import pipeline\n",
    "\n",
    "docker_runtime = SeldonDockerRuntime()\n",
    "\n",
    "sklearn_model = Model(\n",
    "        name=\"test-iris-sklearn\",\n",
    "        runtime=docker_runtime,\n",
    "        platform=ModelFramework.SKLearn,\n",
    "        uri=\"gs://seldon-models/sklearn/iris\"\n",
    ")\n",
    "\n",
    "xgboost_model = Model(\n",
    "        name=\"test-iris-xgboost\",\n",
    "        runtime=docker_runtime,\n",
    "        platform=ModelFramework.XGBoost,\n",
    "        uri=\"gs://seldon-models/xgboost/iris\"\n",
    ")\n",
    "\n",
    "docker_runtime_v2 = SeldonDockerRuntime(protocol=KFServingV2Protocol())\n",
    "\n",
    "@pipeline(name=\"classifier\",\n",
    "          runtime=docker_runtime_v2,\n",
    "          uri=\"gs://seldon-models/test/custom\",\n",
    "          models=[sklearn_model, xgboost_model])\n",
    "def classifier(payload: np.ndarray) -> np.ndarray:\n",
    "    res1 = sklearn_model(payload)\n",
    "\n",
    "    if res1[0][0] > 0.7:\n",
    "        return res1\n",
    "    else:\n",
    "        return xgboost_model(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-variation",
   "metadata": {},
   "source": [
    "## Deploying pipeline to Docker\n",
    "\n",
    "The next step, will be to deploy our pipeline to Docker.\n",
    "We will divide this process into 3 sub-steps:\n",
    "\n",
    "1. Save our artifacts and environment\n",
    "2. Download our model artifacts locally\n",
    "3. Deploy resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-pacific",
   "metadata": {},
   "source": [
    "### Saving artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charged-compensation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packages...\n",
      "Packing environment at '/home/agm/.conda/envs/tempo-4169d184-9e7d-49f8-8130-4f23955e79fd' to '/tmp/tmp_8gs4n82/environment.tar.gz'\n",
      "[########################################] | 100% Completed | 19.0s\n"
     ]
    }
   ],
   "source": [
    "classifier.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-payday",
   "metadata": {},
   "source": [
    "### Downloading model artifacts\n",
    "\n",
    "Since we are going to deploy our pipeline locally using Docker, we'll need to download the model artifacts locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proper-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model.download()\n",
    "xgboost_model.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-sperm",
   "metadata": {},
   "source": [
    "### Deploying pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "iraqi-gasoline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.deploy()\n",
    "classifier.wait_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-tucson",
   "metadata": {},
   "source": [
    "### Sending requests\n",
    "\n",
    "Lastly, we can now send requests to our deployed pipeline.\n",
    "For this, we will leverage the `remote()` method, which will interact without our deployed pipeline (as opposed to executing our pipeline's code locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "signed-approval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.remote(payload=np.array([[1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-orange",
   "metadata": {},
   "source": [
    "### Undeploy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "innocent-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-translator",
   "metadata": {},
   "source": [
    "## Deploying pipeline to K8s\n",
    "\n",
    "The next step, will be to deploy our pipeline to Kubernetes.\n",
    "We will divide this process into 3 sub-steps:\n",
    "\n",
    "1. Save our artifacts and environment\n",
    "2. Upload to remote storage\n",
    "3. Deploy resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-valuation",
   "metadata": {},
   "source": [
    "### Change runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conscious-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "k8s_options = KubernetesOptions(namespace=\"production\")\n",
    "k8s_runtime = SeldonKubernetesRuntime(k8s_options=k8s_options)\n",
    "\n",
    "sklearn_model.set_runtime(k8s_runtime)\n",
    "xgboost_model.set_runtime(k8s_runtime)\n",
    "\n",
    "k8s_runtime_v2 = SeldonKubernetesRuntime(k8s_options=k8s_options, protocol=KFServingV2Protocol())\n",
    "\n",
    "classifier.set_runtime(k8s_runtime_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-azerbaijan",
   "metadata": {},
   "source": [
    "### Saving artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(save_env=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-worry",
   "metadata": {},
   "source": [
    "### Uploading artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "essential-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-bhutan",
   "metadata": {},
   "source": [
    "### Setting up RBAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mexican-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/tempo-pipeline created\n",
      "role.rbac.authorization.k8s.io/tempo-pipeline created\n",
      "rolebinding.rbac.authorization.k8s.io/tempo-pipeline-rolebinding created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../../../tempo/tests/testdata/tempo-pipeline-rbac.yaml -n production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-arizona",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "scenic-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.deploy()\n",
    "classifier.wait_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-listing",
   "metadata": {},
   "source": [
    "### Sending requests\n",
    "\n",
    "Lastly, we can now send requests to our deployed pipeline.\n",
    "For this, we will leverage the `remote()` method, which will interact without our deployed pipeline (as opposed to executing our pipeline's code locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "federal-insulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.remote(payload=np.array([[1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-warehouse",
   "metadata": {},
   "source": [
    "### Undeploy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "narrative-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
